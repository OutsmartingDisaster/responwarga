# üöÄ Respon Warga - Complete Deployment Guide (PostgreSQL + Custom Auth)

## üìã **Current Architecture**
- ‚úÖ **Direct PostgreSQL** - using `pg` pool with custom auth schema (`auth.users`, `auth.sessions`)
- ‚úÖ **Docker support** - `docker-compose.yml` with PostgreSQL 16 + pgAdmin
- ‚úÖ **Session-based auth** - SHA256 hashed tokens stored in `auth.sessions`
- ‚úÖ **Custom grey basemap** - CartoDB Positron (no API key needed!)
- üöß **Drone orthophoto** - planned feature (not yet implemented)

## üìã **Planned Upgrades (Future)**
- üöÅ **Drone orthophoto integration** - optimized for large GeoTIFF files
- üîê **JWT tokens** - migrate from session-based to JWT

---

## üìÇ **Table of Contents**
- [Prerequisites](#prerequisites)
- [Environment Setup](#environment-setup)
- [Database Setup](#database-setup)
- [Authentication System](#authentication-system)
- [Map Services Setup](#map-services-setup)
- [Drone Orthophoto Processing](#drone-orthophoto-processing)
- [Docker Deployment](#docker-deployment)
- [Post-Deployment](#post-deployment)
- [Emergency Operations Features](#emergency-operations-features)
- [Data Sharing & Interoperability](#data-sharing--interoperability)
- [Super Admin Overwatch](#super-admin-overwatch)
- [Troubleshooting](#troubleshooting)

---

## üõ†Ô∏è **Prerequisites**

### **Server Requirements**
- **Docker** 20.10+ & **Docker Compose** v2.10+
- **Node.js** 18.x (containerized, no host installation needed)
- **PostgreSQL** 14+ (containerized)
- **GDAL** 3.6+ (for orthophoto processing, containerized)
- **Minimum Hardware**: 4GB RAM, 2 CPU cores, 20GB storage

### **Development Tools (Optional)**
- **QGIS** or **ArcGIS Pro** (for orthophoto preparation)
- **PostGIS** extension enabled in PostgreSQL

---

## ‚öôÔ∏è **Environment Setup**

### **1. Clone Repository**
```bash
git clone https://github.com/OutsmartingDisaster/responwarga.git
cd responwarga
```

### **2. Current docker-compose.yml**

The project already has a working `docker-compose.yml`:

```yaml
services:
  db:
    image: postgres:16
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER:-responwarga}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-responwarga}
      POSTGRES_DB: ${DB_NAME:-responwarga}
    ports:
      - "${DB_PORT:-54322}:5432"
    volumes:
      - ./.docker/postgres/data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-responwarga}"]
      interval: 10s
      timeout: 5s
      retries: 5

  pgadmin:
    image: dpage/pgadmin4:8
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@responwarga.local
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    depends_on:
      db:
        condition: service_healthy

  app:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env.local
    environment:
      DATABASE_URL: postgres://${DB_USER:-responwarga}:${DB_PASSWORD:-responwarga}@db:5432/${DB_NAME:-responwarga}
    ports:
      - "3535:3000"
    depends_on:
      db:
        condition: service_healthy
    command: npm run dev -- --hostname 0.0.0.0
```

### **3. Future Production docker-compose.yml**

For production with nginx, redis, and orthophoto processing:

```yaml
# docker-compose.prod.yml (PLANNED)
services:
  app:
    build:
      context: .
      target: production
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://responwarga:${POSTGRES_PASSWORD}@postgres:5432/responwarga
    depends_on:
      - postgres
    restart: unless-stopped

  postgres:
    image: postgis/postgis:16-3.4
    environment:
      - POSTGRES_USER=responwarga
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=responwarga
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped

  # PLANNED: nginx reverse proxy
  # PLANNED: orthophoto processor

volumes:
  pgdata:
```

### **4. Current .env.local**

```env
# Database Connection (REQUIRED)
DATABASE_URL=postgresql://USERNAME:PASSWORD@HOST:PORT/DATABASE_NAME

# Individual DB params (used by docker-compose)
DB_USER=responwarga
DB_PASSWORD=your_secure_password
DB_HOST=localhost
DB_PORT=54322
DB_NAME=responwarga

# Map uses CartoDB grey basemap - no API key needed!
```

---

## üóÑÔ∏è **Database Setup**

### **Current Database Schema**

The project uses `auth` schema for authentication. Key tables:

**`auth.users`** - User accounts
```sql
-- Already exists in your database
CREATE TABLE auth.users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) DEFAULT 'user',
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**`auth.sessions`** - Session tokens (SHA256 hashed)
```sql
CREATE TABLE auth.sessions (
    token_hash VARCHAR(64) PRIMARY KEY,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    expires_at TIMESTAMPTZ NOT NULL,
    revoked_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**`profiles`** - User profiles linked to organizations
```sql
CREATE TABLE profiles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES auth.users(id),
    name VARCHAR(255),
    username VARCHAR(100),
    role VARCHAR(50) DEFAULT 'responder',
    organization_id UUID REFERENCES organizations(id),
    organization VARCHAR(255),
    phone VARCHAR(20),
    status VARCHAR(20)
);
```

### **Migration Files Location**

All migrations are in `/db/migrations/`:

```
db/migrations/
‚îú‚îÄ‚îÄ 20251202_001_response_operations.sql  # Response ops, team, field reports
‚îú‚îÄ‚îÄ 20251204_001_crowdsource.sql          # Crowdsource projects & submissions
‚îú‚îÄ‚îÄ 20251204_002_crowdsource_geofence_level.sql
‚îú‚îÄ‚îÄ 20251204_003_crowdsource_form_fields.sql
‚îú‚îÄ‚îÄ 20251204_004_crowdsource_multi_geofence.sql
‚îú‚îÄ‚îÄ 20251204_005_form_fields_media.sql
‚îú‚îÄ‚îÄ 20251204_006_location_uncertain.sql
‚îú‚îÄ‚îÄ 20251204_007_map_layers.sql
‚îú‚îÄ‚îÄ 20251204_008_add_field_types.sql
‚îî‚îÄ‚îÄ 20251204_009_consent_publish_name.sql
```

### **Run Migrations**

```bash
# Run all migrations
for f in db/migrations/*.sql; do
  echo "Running: $f"
  node scripts/run-migration.js "$f"
done

# Or use combined file
psql $DATABASE_URL -f db/all_migrations.sql
```

### **1. Database Schema Files (PLANNED)**

Future auth schema improvements:

**`001_create_auth_schema.sql` (PLANNED)**
```sql
-- Future: JWT-based auth with refresh tokens
CREATE TABLE IF NOT EXISTS refresh_tokens (
    token VARCHAR(255) PRIMARY KEY,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    expires_at TIMESTAMPTZ NOT NULL,
    revoked BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS user_roles (
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    role_id INTEGER REFERENCES roles(id) ON DELETE CASCADE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (user_id, role_id)
);

```

---

## üîê **Authentication System**

### **Current Implementation**

The project uses **session-based authentication** with SHA256 hashed tokens.

**Key files:**
- `/src/lib/auth/session.ts` - Session management
- `/src/lib/auth/password.ts` - Password hashing (bcrypt)
- `/src/lib/auth/api.ts` - Client-side auth API
- `/src/lib/auth/rbac.ts` - Role-based access control

**Session flow:**
1. User logs in ‚Üí password verified with bcrypt
2. Random 48-byte token generated
3. Token hashed with SHA256, stored in `auth.sessions`
4. Plain token set as HTTP-only cookie (`rw_session`)
5. On each request, cookie token is hashed and matched against DB

### **Current Auth Code** (excerpt from `/src/lib/auth/session.ts`):
```typescript
import crypto from 'crypto'
import { query } from '@/lib/db/pool'

const SESSION_COOKIE_NAME = 'rw_session'
const SESSION_MAX_AGE_DAYS = 7

function hashToken(token: string) {
  return crypto.createHash('sha256').update(token).digest('hex')
}

export async function createSession(userId: string) {
  const token = crypto.randomBytes(48).toString('hex')
  const tokenHash = hashToken(token)
  const expiresAt = new Date(Date.now() + SESSION_MAX_AGE_DAYS * 24 * 60 * 60 * 1000)

  await query(
    'INSERT INTO auth.sessions (token_hash, user_id, expires_at) VALUES ($1, $2, $3)',
    [tokenHash, userId, expiresAt.toISOString()]
  )
  return { token, expiresAt }
}

export async function getSessionFromCookies(cookieSource) {
  const token = cookieSource.get(SESSION_COOKIE_NAME)?.value
  if (!token) return null
  
  const tokenHash = hashToken(token)
  const { rows } = await query(
    `SELECT s.user_id, u.email, u.role, p.* 
     FROM auth.sessions s
     JOIN auth.users u ON u.id = s.user_id
     LEFT JOIN profiles p ON p.user_id = s.user_id
     WHERE s.token_hash = $1 AND s.expires_at > NOW() AND s.revoked_at IS NULL`,
    [tokenHash]
  )
  return rows[0] || null
}
```

JWT-based auth with refresh tokens is planned for future upgrade.

---

## üó∫Ô∏è **Map Services Setup**

### **Custom Grey Basemap (Implemented)**

The project uses **CartoDB Positron** - a free grey basemap with no API key required!

**Configuration:** `/src/lib/map/config.ts`

```typescript
// Available basemaps (all free, no API key needed)
export const BASEMAPS = {
  grey: {
    url: 'https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png',
    attribution: '¬© OpenStreetMap ¬© CARTO',
    subdomains: 'abcd',
  },
  dark: {
    url: 'https://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}{r}.png',
    attribution: '¬© OpenStreetMap ¬© CARTO',
    subdomains: 'abcd',
  },
};

export const DEFAULT_BASEMAP = BASEMAPS.grey;  // Light grey for forms
export const DARK_BASEMAP = BASEMAPS.dark;     // Dark for dashboards
```

**Usage in components:**
```typescript
import { DEFAULT_BASEMAP } from '@/lib/map/config';

L.tileLayer(DEFAULT_BASEMAP.url, {
  attribution: DEFAULT_BASEMAP.attribution,
  subdomains: DEFAULT_BASEMAP.subdomains,
}).addTo(map);
```

**Updated components:**
- `/src/components/crowdsourcing/ProjectMapView.tsx` - grey basemap
- `/src/components/crowdsourcing/MapComponent.tsx` - grey basemap
- `/src/app/components/MapLeaflet.tsx` - dark basemap (dashboard)
- `/src/app/components/MiniMapPicker.tsx` - grey basemap
- `/src/app/responder/dashboard/operations/MiniMapPicker.tsx` - grey basemap

---

## üöÅ **Drone Orthophoto Processing** (PLANNED)

### **1. Processing Service Dockerfile**
Create `Dockerfile.ortho`:
```dockerfile
FROM osgeo/gdal:ubuntu-small-3.8

# Install Node.js and dependencies
RUN apt-get update && apt-get install -y \
    curl \
    gnupg \
    nodejs \
    npm \
    python3-pip \
    python3-setuptools \
    && rm -rf /var/lib/apt/lists/*

# Install Node.js packages
WORKDIR /app
COPY package*.json ./
RUN npm install geotiff georaster-layer-for-leaflet @mapbox/tilebelt

# Copy processing scripts
COPY scripts/ortho-processing.js /app/scripts/
COPY scripts/utils.js /app/scripts/

# Set up entrypoint
COPY scripts/entrypoint.sh /app/scripts/
RUN chmod +x /app/scripts/entrypoint.sh

EXPOSE 3001
ENTRYPOINT ["/app/scripts/entrypoint.sh"]
```

### **2. Orthophoto Processing Script**
Create `/scripts/ortho-processing.js`:
```javascript
const { exec } = require('child_process');
const { promisify } = require('util');
const fs = require('fs').promises;
const path = require('path');
const { db } = require('../src/lib/db');
const { processGeotiff, generateThumbnail } = require('./utils');

const execAsync = promisify(exec);

async function processOrthophoto(orthoId, filePath) {
  try {
    console.log(`Starting processing for orthophoto ${orthoId}`);
    
    // Get ortho record
    const [ortho] = await db.query(
      `SELECT file_name, disaster_response_id FROM orthophotos WHERE id = $1`,
      [orthoId]
    );
    
    if (!ortho) {
      throw new Error(`Orthophoto ${orthoId} not found`);
    }
    
    const baseName = path.basename(filePath, path.extname(filePath));
    const outputDir = path.dirname(filePath);
    const cogPath = path.join(outputDir, `${baseName}.cog.tif`);
    const thumbPath = path.join(outputDir, `${baseName}_thumb.jpg`);
    
    // Step 1: Convert to Cloud Optimized GeoTIFF
    console.log('Converting to COG...');
    await execAsync(`
      gdal_translate -of COG \
        -co COMPRESS=WEBP \
        -co QUALITY=85 \
        -co BLOCKSIZE=512 \
        -co OVERVIEWS=IGNORE_EXISTING \
        -co SPARSE_OK=YES \
        "${filePath}" "${cogPath}"
    `);
    
    // Step 2: Generate thumbnail
    console.log('Generating thumbnail...');
    await generateThumbnail(filePath, thumbPath, 1024);
    
    // Step 3: Get bounds from COG
    console.log('Extracting bounds...');
    const bounds = await processGeotiff(cogPath);
    
    // Step 4: Calculate file sizes
    const originalStats = await fs.stat(filePath);
    const cogStats = await fs.stat(cogPath);
    
    const originalSize = originalStats.size / (1024 * 1024); // MB
    const processedSize = cogStats.size / (1024 * 1024); // MB
    
    // Step 5: Update database
    console.log('Updating database...');
    await db.query(`
      UPDATE orthophotos 
      SET 
        processing_status = 'completed',
        cog_url = $1,
        thumbnail_url = $2,
        bounds = ST_MakeEnvelope($3, $4, $5, $6, 4326),
        original_size_mb = $7,
        processed_size_mb = $8,
        updated_at = NOW()
      WHERE id = $9
    `, [
      `/orthophotos/${path.basename(cogPath)}`,
      `/orthophotos/${path.basename(thumbPath)}`,
      bounds.minLng, bounds.minLat, bounds.maxLng, bounds.maxLat,
      originalSize.toFixed(2),
      processedSize.toFixed(2),
      orthoId
    ]);
    
    console.log(`Orthophoto ${orthoId} processing completed successfully`);
    
    // Step 6: Clean up original file
    await fs.unlink(filePath);
    console.log('Original file cleaned up');
    
    return {
      success: true,
      cogUrl: `/orthophotos/${path.basename(cogPath)}`,
      thumbnailUrl: `/orthophotos/${path.basename(thumbPath)}`,
      bounds
    };
    
  } catch (error) {
    console.error(`Processing failed for ${orthoId}:`, error);
    
    // Update status to failed
    await db.query(`
      UPDATE orthophotos 
      SET processing_status = 'failed', 
          updated_at = NOW() 
      WHERE id = $1
    `, [orthoId]);
    
    throw error;
  }
}

// Process queue listener
async function startProcessingQueue() {
  console.log('Starting orthophoto processing queue...');
  
  while (true) {
    try {
      // Get pending orthophotos
      const pendingOrthos = await db.query(`
        SELECT id, file_path 
        FROM orthophotos 
        WHERE processing_status = 'pending' 
        ORDER BY created_at ASC 
        LIMIT 1
        FOR UPDATE SKIP LOCKED
      `);
      
      if (pendingOrthos.length > 0) {
        const ortho = pendingOrthos[0];
        await processOrthophoto(ortho.id, ortho.file_path);
      }
      
      // Sleep for 5 seconds before checking again
      await new Promise(resolve => setTimeout(resolve, 5000));
      
    } catch (error) {
      console.error('Queue processing error:', error);
      await new Promise(resolve => setTimeout(resolve, 10000)); // Longer sleep on error
    }
  }
}

// Start the processor
startProcessingQueue().catch(console.error);
```

### **3. Utility Functions**
Create `/scripts/utils.js`:
```javascript
const { exec } = require('child_process');
const { promisify } = require('util');
const execAsync = promisify(exec);
const { GeoTIFF } = require('geotiff');
const fs = require('fs').promises;
const path = require('path');

async function processGeotiff(filePath) {
  try {
    const tiff = await GeoTIFF.fromFile(filePath);
    const image = await tiff.getImage();
    const geoKeys = await image.getGeoKeys();
    const [width, height] = await image.getWidth(), await image.getHeight();
    
    // Get corner coordinates
    const [minX, minY, maxX, maxY] = await image.getBoundingBox();
    
    return {
      minLng: minX,
      minLat: minY,
      maxLng: maxX,
      maxLat: maxY,
      width,
      height,
      projection: geoKeys.ProjectedCSTypeGeoKey || 4326,
      pixelSize: await image.getFileDirectory().ModelPixelScale
    };
  } catch (error) {
    console.error('Error processing GeoTIFF:', error);
    throw error;
  }
}

async function generateThumbnail(inputPath, outputPath, maxSize) {
  try {
    await execAsync(`
      gdal_translate -outsize ${maxSize} 0 -scale -ot Byte \
        -co QUALITY=75 \
        "${inputPath}" "${outputPath}"
    `);
    console.log(`Thumbnail generated: ${outputPath}`);
  } catch (error) {
    console.error('Thumbnail generation failed:', error);
    throw error;
  }
}

async function validateGeotiff(filePath) {
  try {
    const result = await execAsync(`gdalinfo "${filePath}"`);
    console.log('GeoTIFF validation passed');
    return true;
  } catch (error) {
    console.error('GeoTIFF validation failed:', error);
    return false;
  }
}

module.exports = {
  processGeotiff,
  generateThumbnail,
  validateGeotiff
};
```

---

## üê≥ **Docker Deployment**

### **1. Application Dockerfile**
Create `Dockerfile`:
```dockerfile
# Base image
FROM node:18-alpine AS base

# Install dependencies first (better caching)
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

# Copy application code
COPY . .

# Build Next.js app
RUN npm run build

# Production image (multi-stage build)
FROM node:18-alpine AS production
WORKDIR /app

# Create non-root user for security
RUN addgroup -g 1001 -S nodejs && \
    adduser -S responwarga -u 1001 && \
    chown -R responwarga:nodejs /app

# Copy built app and node_modules
COPY --from=base /app/.next ./.next
COPY --from=base /app/node_modules ./node_modules
COPY --from=base /app/package*.json ./
COPY --from=base /app/public ./public

USER responwarga
EXPOSE 3000

# Health check endpoint for monitoring
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:3000/api/health || exit 1

# Start with graceful shutdown
CMD ["npm", "start"]
```

### **2. Docker Build & Run Commands**
```bash
# Build all services
docker-compose build

# Start services in background
docker-compose up -d

# View logs
docker-compose logs -f app

# Run database migrations
docker-compose exec app npx prisma migrate deploy

# Create admin user
docker-compose exec app node scripts/create-admin.js

# Check service health
docker-compose ps
```

### **3. Admin User Creation Script**
Create `/scripts/create-admin.js`:
```javascript
const { db } = require('../src/lib/db');
const { hash } = require('bcryptjs');
const { v4: uuidv4 } = require('uuid');

async function createAdminUser() {
  try {
    const email = 'admin@responwarga.id';
    const password = 'secure_password_123'; // Change this in production!
    const fullName = 'System Administrator';
    
    // Check if admin exists
    const [existingAdmin] = await db.query(
      'SELECT id FROM users WHERE email = $1',
      [email]
    );
    
    if (existingAdmin) {
      console.log('Admin user already exists');
      return;
    }
    
    // Create user
    const passwordHash = await hash(password, 12);
    const userId = uuidv4();
    
    await db.query(`
      INSERT INTO users (id, email, password_hash, full_name, is_active)
      VALUES ($1, $2, $3, $4, true)
    `, [userId, email, passwordHash, fullName]);
    
    // Get admin role
    const [adminRole] = await db.query(
      'SELECT id FROM roles WHERE name = $1',
      ['super_admin']
    );
    
    if (!adminRole) {
      throw new Error('Admin role not found');
    }
    
    // Assign admin role
    await db.query(`
      INSERT INTO user_roles (user_id, role_id)
      VALUES ($1, $2)
    `, [userId, adminRole.id]);
    
    // Create sample organization
    const orgId = uuidv4();
    await db.query(`
      INSERT INTO organizations (id, name, slug, status, created_by)
      VALUES ($1, $2, $3, 'active', $4)
    `, [orgId, 'BPBD Pusat', 'bpbd-pusat', userId]);
    
    console.log('‚úÖ Admin user created successfully');
    console.log('üìß Email:', email);
    console.log('üîí Password:', password);
    console.log('üè¢ Organization created: BPBD Pusat');
    
  } catch (error) {
    console.error('‚ùå Failed to create admin user:', error);
    process.exit(1);
  } finally {
    await db.end();
    process.exit(0);
  }
}

createAdminUser();
```

---

## ‚úÖ **Post-Deployment**

### **1. SSL Certificate Setup (Let's Encrypt)**
```bash
# Install Certbot
sudo apt install certbot python3-certbot-nginx

# Get SSL certificate
sudo certbot --nginx -d responwarga.yourdomain.com

# Auto-renewal setup
sudo certbot renew --dry-run
```

### **2. Backup Strategy**
Create `/scripts/backup.sh`:
```bash
#!/bin/bash
set -e

# Backup configuration
BACKUP_DIR="/backups"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
DB_BACKUP="${BACKUP_DIR}/responwarga_db_${TIMESTAMP}.sql.gz"
FILES_BACKUP="${BACKUP_DIR}/responwarga_files_${TIMESTAMP}.tar.gz"
RETENTION_DAYS=7

# Create backup directory
mkdir -p $BACKUP_DIR

echo "üîÑ Starting backup process..."

# Database backup
echo "üíæ Backing up database..."
docker-compose exec -T postgres pg_dump -U responwarga responwarga | gzip > $DB_BACKUP

# Files backup (orthophotos, map styles)
echo "üìÅ Backing up files..."
tar -czf $FILES_BACKUP -C /var/lib/docker/volumes/responwarga_pgdata/_data/orthophotos .

# Upload to cloud storage (optional)
# aws s3 cp $DB_BACKUP s3://responwarga-backups/
# aws s3 cp $FILES_BACKUP s3://responwarga-backups/

# Cleanup old backups
echo "üßπ Cleaning up old backups (keeping last ${RETENTION_DAYS} days)..."
find $BACKUP_DIR -name "responwarga_db_*.sql.gz" -mtime +$RETENTION_DAYS -delete
find $BACKUP_DIR -name "responwarga_files_*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "‚úÖ Backup completed successfully!"
echo "üìä Database backup: $DB_BACKUP"
echo "üìÅ Files backup: $FILES_BACKUP"
```

Make executable and schedule:
```bash
chmod +x scripts/backup.sh
# Add to crontab: 0 2 * * * /path/to/scripts/backup.sh
```

### **3. Monitoring Setup**
Create `/src/pages/api/health.ts`:
```typescript
import { NextApiRequest, NextApiResponse } from 'next';
import { db } from '../../lib/db';
import redis from '../../lib/redis';

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  try {
    // Database check
    const dbCheck = await db.query('SELECT 1');
    
    // Redis check
    const redisCheck = await redis.ping();
    
    // Storage check (orthophotos directory)
    const fs = require('fs');
    const orthoPath = '/app/public/orthophotos';
    const storageCheck = fs.existsSync(orthoPath) && fs.readdirSync(orthoPath).length > 0;
    
    res.status(200).json({
      status: 'healthy',
      timestamp: new Date().toISOString(),
      uptime: process.uptime(),
      dependencies: {
        database: dbCheck.rowCount > 0 ? 'ok' : 'failed',
        cache: redisCheck === 'PONG' ? 'ok' : 'failed',
        storage: storageCheck ? 'ok' : 'failed'
      },
      version: process.env.npm_package_version || 'unknown'
    });
  } catch (error) {
    console.error('Health check failed:', error);
    res.status(503).json({
      status: 'unhealthy',
      error: error.message,
      timestamp: new Date().toISOString()
    });
  }
}
```

---

## üö® **Emergency Operations Features**

### **1. Live Incident Room**
- Central incident timeline with filters by disaster type, severity, and region.
- Real-time feed combining field reports, crowdsource submissions, and orthophotos.
- Quick actions for acknowledging, escalating, and closing incidents.

### **2. Field Team Tasking & Status**
- Assignment view showing which team handles which incident or area.
- Status tracking (assigned, en route, on scene, completed) with timestamps.
- Simple SLAs for critical incident types (for example, life-threatening within X minutes).

### **3. Priority & Escalation Rules**
- Configurable rules to auto-flag high-risk reports (keywords, location, vulnerable groups).
- Escalation paths from responder ‚Üí coordinator ‚Üí command level.
- Audit trail of all actions taken on a critical incident.

---

## üîÑ **Data Sharing & Interoperability**

### **1. Secure Data Export**
- Role-based exports (CSV/GeoJSON) for selected time ranges and geofences.
- Automatic anonymization for sensitive fields when exporting outside the organization.
- Export presets for common reporting needs (daily situation report, weekly summary).

### **2. Inter-Agency APIs**
- Read-only API keys for partner agencies with scoped access per organization or geofence.
- Standardized JSON and GeoJSON responses for easy integration into other systems.
- Webhook subscriptions for high-priority events (for example, new emergency report in a shared area).

### **3. Open Data Windows (Anonymized)**
- Optional public endpoints for non-sensitive, aggregated statistics.
- Clear configuration to decide what can be shared (for example, counts per district, not raw reports).
- Built-in metadata (license, last updated, coverage) for transparency.

---

## üõ∞Ô∏è **Super Admin Overwatch (Multi-Organization)**

### **1. Global Situation Dashboard**
- Cross-organization map and metrics (active incidents, response times, open tasks).
- Filters by province, organization, hazard type, and time window.
- Ability to drill down from national view ‚Üí province ‚Üí single organization dashboard.

### **2. Organization Health & Compliance**
- High-level indicators per organization (login activity, data freshness, backlog, error rates).
- Signals for under-reporting or overload (for example, few reports but major disaster declared).
- Periodic summary reports for leadership across all organizations.

### **3. Cross-Org Access Controls & Audit Trails**
- Dedicated `super_admin` role with read-only access across organizations by default.
- Granular overrides for what super_admins can change (for example, only configuration, not raw data).
- Detailed audit logs of super_admin actions for accountability and compliance.

---

## üö® **Troubleshooting**

### **Common Issues & Solutions**

#### **1. Database Connection Issues**
```bash
# Check PostgreSQL container status
docker-compose ps postgres

# View PostgreSQL logs
docker-compose logs postgres

# Test database connection from app container
docker-compose exec app psql -h postgres -U responwarga -d responwarga -c "SELECT 1"
```

#### **2. Orthophoto Processing Failures**
```bash
# Check ortho-processor logs
docker-compose logs ortho-processor

# Validate GeoTIFF file manually
docker-compose exec ortho-processor gdalinfo /app/public/orthophotos/test.tif

# Check disk space
docker-compose exec postgres df -h
```

#### **3. Map Loading Issues**
```bash
# Check if map styles are accessible
curl http://localhost/map-styles/grey-basemap/style.json

# Verify Nginx configuration
docker-compose exec nginx nginx -t

# Check browser console for map errors
# Look for CORS issues or 404 errors for map tiles
```

#### **4. Authentication Problems**
```bash
# Verify JWT secret is set
docker-compose exec app env | grep JWT_SECRET

# Check user exists in database
docker-compose exec postgres psql -U responwarga -d responwarga -c "SELECT email, is_active FROM users WHERE email = 'admin@responwarga.id'"

# Test login endpoint
curl -X POST http://localhost/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@responwarga.id","password":"secure_password_123"}'
```

### **Emergency Recovery Procedures**

#### **1. Database Restore**
```bash
# Stop services
docker-compose stop app

# Restore database from backup
gunzip -c /backups/responwarga_db_20251204_120000.sql.gz | docker-compose exec -T postgres psql -U responwarga -d responwarga

# Start services
docker-compose start app
```

#### **2. Container Recreation**
```bash
# Remove all containers (keeps volumes)
docker-compose down

# Rebuild and start
docker-compose up -d --build
```

#### **3. File System Recovery**
```bash
# Restore orthophotos from backup
tar -xzf /backups/responwarga_files_20251204_120000.tar.gz -C /path/to/orthophotos
```

---

## üéØ **Performance Optimization Checklist**

### **Database Optimization**
- [ ] Enable connection pooling with PgBouncer
- [ ] Create appropriate indexes for frequently queried columns
- [ ] Set up autovacuum for large tables
- [ ] Partition orthophotos table by disaster_response_id
- [ ] Regular analyze and vacuum operations

### **Map Performance**
- [ ] Implement tile caching 
- [ ] Pre-generate orthophoto overviews
- [ ] Use WebP compression for thumbnails
- [ ] Implement lazy loading for off-screen areas
- [ ] Add loading indicators for large orthophotos

### **Application Performance**
- [ ] Enable Next.js ISR (Incremental Static Regeneration)
- [ ] Implement API response caching
- [ ] Use Redis for session management
- [ ] Optimize React component rendering
- [ ] Implement code splitting for large components

### **Infrastructure Scaling**
- [ ] Set up read replicas for database
- [ ] Implement load balancing for multiple app instances
- [ ] Use CDN for static assets and orthophotos
- [ ] Set up auto-scaling based on CPU/memory usage
- [ ] Implement circuit breakers for external service calls

---

## üí° **Final Recommendations**

### **1. Security Best Practices**
- **Never commit** `.env` files to version control
- Use **separate environments** for development, staging, and production
- Implement **rate limiting** on API endpoints
- Regularly **rotate JWT secrets** and database passwords
- Set up **automated security scanning** for dependencies

### **2. Disaster Response Specific Optimizations**
- **Offline-first capability**: Implement service workers for offline form submission
- **SMS fallback**: Add Twilio integration for areas with poor connectivity
- **Low-bandwidth mode**: Serve compressed images and simplified UI for 2G/3G networks
- **Emergency priority queue**: Separate queue for life-threatening reports
- **Multi-language support**: Add Bahasa Indonesia and local dialects

### **3. Cost Optimization Strategy**
```
Estimated monthly costs after optimizations:
- Server (4GB RAM, 2 CPU): $40 (DigitalOcean)
- Storage (100GB): $10 (Block storage)
- Bandwidth: $5 (1TB transfer)
- Backup storage: $3 (S3 Infrequent Access)
- Monitoring: $0 (self-hosted Prometheus/Grafana)
TOTAL: ~$58/month
```

---

## ‚úÖ **Summary: Current vs Planned**

| Feature | Current Status | Planned |
|---------|---------------|---------|
| **Database** | ‚úÖ PostgreSQL 16 via `pg` pool | PostGIS for spatial queries |
| **Auth** | ‚úÖ Session-based (SHA256 tokens) | JWT with refresh tokens |
| **Maps** | ‚úÖ CartoDB grey basemap (no API key!) | - |
| **Orthophoto** | üöß Not implemented | GDAL COG processing |
| **Deployment** | ‚úÖ Docker Compose | Full production stack with nginx |
| **Caching** | üöß None | Redis + nginx proxy cache |

This guide documents both the **current implementation** and **planned upgrades** for Respon Warga. üö®